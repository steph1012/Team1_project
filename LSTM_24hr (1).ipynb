{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3PIACVZVC-o",
    "outputId": "6f37a453-d93d-471c-b8e8-c1318b157fb3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# %cd \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YxpOJjdPUcBX"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 25 11:13:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
      "| 60%   52C    P2             133W / 370W |  22684MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:B3:00.0 Off |                  N/A |\n",
      "|  0%   34C    P8              20W / 370W |    262MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1314493      C   ...yan1211/.conda/envs/test/bin/python     1244MiB |\n",
      "|    0   N/A  N/A   2831241      C   ...na10/.conda/envs/myenv_3/bin/python    21430MiB |\n",
      "|    1   N/A  N/A   1314493      C   ...yan1211/.conda/envs/test/bin/python      254MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "tf.config.set_visible_devices(devices=gpus[0], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPdZl-OmUcBa",
    "outputId": "6f8a2ee1-907a-40fd-de09-bc0833802788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18295020351171411350\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1142882304\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2010853718315646423\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23283367936\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13759530565716513981\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 2144165316\n",
      "]\n",
      "===============\n",
      "2.1.2+cu121\n",
      "===============\n",
      "12.1\n",
      "===============\n",
      "8902\n",
      "===============\n",
      "2.8.0\n",
      "===============\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 11:13:35.205345: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-25 11:13:35.348277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 1089 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2023-12-25 11:13:35.348856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:1 with 22204 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "print(\"===============\")\n",
    "print(torch.__version__)\n",
    "print(\"===============\")\n",
    "print(torch.version.cuda)\n",
    "print(\"===============\")\n",
    "print(torch.backends.cudnn.version())\n",
    "print(\"===============\")\n",
    "print(tf.__version__)\n",
    "print(\"===============\")\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ujURKlkYUcBc"
   },
   "outputs": [],
   "source": [
    "# Fix the seed for reproducibility\n",
    "seed = 999\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix_nyo4tUcBc"
   },
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = 'csv.tfrecords'\n",
    "\n",
    "# raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "# X_train = []\n",
    "# Y_train = []\n",
    "# i=0\n",
    "# for raw_record in raw_dataset:\n",
    "#     sub_y = []\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(raw_record.numpy())\n",
    "#     print(example)\n",
    "#     # print(example.features.feature['icu_intime'].bytes_list)\n",
    "#     # sub_y.append(0 if example.features.feature['gender'].float_list.value[0] == 0 else 1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiKuBz7kUcBd",
    "outputId": "ba999f94-d13a-4eb0-b98e-28146f6cae89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index   stay_id   hadm_id  subject_id  label  \\\n",
      "0           0      0  30039798  27880403    18499026      0   \n",
      "1           1      0  30039798  27880403    18499026      0   \n",
      "2           2      0  30039798  27880403    18499026      0   \n",
      "3           3      0  30039798  27880403    18499026      0   \n",
      "4           4      0  30039798  27880403    18499026      0   \n",
      "\n",
      "            icu_intime  charttime  urineoutput          icu_outtime  \\\n",
      "0  2137-09-15 08:36:00          0          0.0  2137-09-16 17:28:21   \n",
      "1  2137-09-15 08:36:00          1        185.0  2137-09-16 17:28:21   \n",
      "2  2137-09-15 08:36:00          2         82.0  2137-09-16 17:28:21   \n",
      "3  2137-09-15 08:36:00          3        220.0  2137-09-16 17:28:21   \n",
      "4  2137-09-15 08:36:00          4        140.0  2137-09-16 17:28:21   \n",
      "\n",
      "   aki_stage  aki_charttime  rank  \n",
      "0        0.0              0     1  \n",
      "1        0.0              1     1  \n",
      "2        0.0              2     1  \n",
      "3        0.0              3     1  \n",
      "4        0.0              4     1  \n"
     ]
    }
   ],
   "source": [
    "df  = pd.read_csv(\"final_result.csv\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/cpu:0'):\n",
    "\n",
    "#     filename = 'csv.tfrecords'\n",
    "    \n",
    "#     raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "#     # uo = data['urineoutput']\n",
    "#     # aki_stage = data['aki_stage']\n",
    "#     # charttime = data['charttime']\n",
    "#     uo = []\n",
    "#     aki_stage = []\n",
    "#     charttime = []\n",
    "#     stay_id_list = set()\n",
    "#     i=0\n",
    "#     for raw_record in raw_dataset:\n",
    "#         sub_y = []\n",
    "    \n",
    "#         example = tf.train.Example()\n",
    "#         example.ParseFromString(raw_record.numpy())\n",
    "#         stay_id_list.add(example.features.feature['stay_id'].float_list.value[0])\n",
    "#         # uo.append(example.features.feature['urineoutput'].float_list.value[0])\n",
    "#         # aki_stage.append(example.features.feature['aki_stage'].float_list.value[0])\n",
    "#         # charttime.append(example.features.feature['charttime'].float_list.value[0])\n",
    "        \n",
    "#     print(len(stay_id_list))\n",
    "#     # print(len(uo))\n",
    "#     # print(len(stay_id_list))\n",
    "#     # X_train = np.array(X_train)\n",
    "#     # Y_train = np.array(Y_train)\n",
    "#     # X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],-1)\n",
    "#     # Y_train = Y_train.astype(np.float64)\n",
    "#     # print(Y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOfoif1sUcBe"
   },
   "source": [
    "### padding data of each patient into the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHENiGVCUcBe",
    "outputId": "1356d4d3-96d9-4c64-f9a0-53d48c4af54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totally 49669 patients\n"
     ]
    }
   ],
   "source": [
    "stay_id_list = df[\"stay_id\"].unique()\n",
    "print(\"there are totally {} patients\".format(len(stay_id_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters for processing\n",
    "# padding_hr = 23\n",
    "# predict_after_hr = 6\n",
    "# X_per_patient = []\n",
    "# Y_per_patient = []\n",
    "\n",
    "# # Process data for each stay_id\n",
    "# for stay_id, data in stay_id_data.items():\n",
    "#     uo = data['urineoutput']\n",
    "#     aki_stage = data['aki_stage']\n",
    "#     charttime = data['charttime']\n",
    "\n",
    "#     if len(aki_stage) <= padding_hr + predict_after_hr:\n",
    "#         continue\n",
    "\n",
    "#     for i in range(len(aki_stage)):\n",
    "#         if i + padding_hr > len(aki_stage) - 1:\n",
    "#             break\n",
    "#         aki_tmp = aki_stage[padding_hr + predict_after_hr]\n",
    "\n",
    "#         uo_tmp = np.array(uo[i: i + padding_hr + 1])\n",
    "#         time_tmp = np.array(charttime[i: i + padding_hr + 1])\n",
    "\n",
    "#         uo_tmp = uo_tmp[:, np.newaxis]\n",
    "#         time_tmp = time_tmp[:, np.newaxis]\n",
    "#         x_tmp = np.hstack((uo_tmp, time_tmp))\n",
    "\n",
    "#         X_per_patient.append(x_tmp)\n",
    "#         Y_per_patient.append(aki_tmp)\n",
    "\n",
    "# # Convert to NumPy arrays\n",
    "# X_per_patient = np.array(X_per_patient, dtype=object)\n",
    "# Y_per_patient = np.array(Y_per_patient, dtype=object)\n",
    "\n",
    "# print(\"After processing, there are {} patients' data\".format(len(X_per_patient)))\n",
    "# print(\"X_per_patient shape:\", X_per_patient.shape)\n",
    "# print(\"Y_per_patient shape:\", Y_per_patient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MlZN7NxjUcBf",
    "outputId": "6e09b0a9-a16f-4c0b-88e2-0ed2de267e9f"
   },
   "outputs": [],
   "source": [
    "# padding_hr = 23          # the first `padding_hr` nodes have no output\n",
    "# predict_after_hr = 6    # for each node, input the nth Hour urineOutput, output the (n + predict_after_hr)th Hour AKI_stage\n",
    "\n",
    "# X_per_patient = [] # urine output from 0H ~ (N - predict_after_hr)H\n",
    "# Y_per_patient = [] # AKI stage from (padding_hr + predict_after_hr)H ~ (N)H\n",
    "# for stay_id in stay_id_list:\n",
    "#     # for each patient\n",
    "#     now_process = df.loc[df['stay_id'] == stay_id].reset_index()\n",
    "\n",
    "#     uo = now_process[\"urineoutput\"].tolist()\n",
    "#     aki_stage = now_process[\"aki_stage\"].tolist()\n",
    "#     charttime = now_process[\"charttime\"].tolist()\n",
    "\n",
    "#     if len(aki_stage) <=  padding_hr + predict_after_hr: # there is no enough data for predict\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     ## sliding window, split each 24 hour as a data\n",
    "#     for i in range(len(aki_stage)):\n",
    "#         if i + padding_hr > len(aki_stage) - 1:\n",
    "#             break\n",
    "#         # aki_tmp = aki_stage[padding_hr + predict_after_hr]\n",
    "#         # if len(uo) - len(aki_stage) != padding_hr:\n",
    "#         #     print(\"err\")\n",
    "\n",
    "#         aki_stage = np.array(aki_stage)\n",
    "#         aki_stage[aki_stage == 2] = 1\n",
    "#         aki_stage[aki_stage == 3] = 1\n",
    "#         aki_stage[aki_stage == 4] = 2\n",
    "        \n",
    "#         uo_tmp = np.array(uo[i : i + padding_hr + 1])\n",
    "#         time_tmp = np.array(charttime[i : i + padding_hr + 1])\n",
    "\n",
    "#         uo_tmp = uo_tmp[:, np.newaxis]\n",
    "#         time_tmp = time_tmp[:, np.newaxis]\n",
    "#         x_tmp = np.hstack((uo_tmp, time_tmp))\n",
    "\n",
    "#         X_per_patient.append(x_tmp)\n",
    "#         # Y_per_patient.append(aki_tmp)\n",
    "#         Y_per_patient.append(aki_stage)\n",
    "\n",
    "# print(\"after process, there are {} patients' data\".format(len(X_per_patient)))\n",
    "\n",
    "\n",
    "# X_per_patient = np.array(X_per_patient)\n",
    "# Y_per_patient = np.array(Y_per_patient)\n",
    "\n",
    "# # the (dim of X) should be the (dim of Y) - padding_hr\n",
    "# print(X_per_patient.shape)\n",
    "# print(Y_per_patient.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing, there are 2499542 patients' data\n",
      "(2499542, 24, 2)\n",
      "(2499542,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "padding_hr = 23          # the first `padding_hr` nodes have no output\n",
    "predict_after_hr = 6  \n",
    "X_per_patient = []  # urine output from 0H ~ (N - predict_after_hr)H\n",
    "Y_per_patient = []  # AKI stage from (padding_hr + predict_after_hr)H ~ (N)H\n",
    "\n",
    "for stay_id in stay_id_list:\n",
    "    # for each patient\n",
    "    now_process = df.loc[df['stay_id'] == stay_id].reset_index()\n",
    "\n",
    "    uo = now_process[\"urineoutput\"].tolist()\n",
    "    aki_stage = np.array(now_process[\"aki_stage\"].tolist())\n",
    "    charttime = now_process[\"charttime\"].tolist()\n",
    "\n",
    "    if len(aki_stage) <= padding_hr + predict_after_hr:  # there is not enough data for prediction\n",
    "        continue\n",
    "\n",
    "    # Adjust AKI stages: Set 2 and 3 to 1, assuming other values are already 0 or 1\n",
    "    aki_stage[(aki_stage == 2) | (aki_stage == 3)] = 1\n",
    "\n",
    "    for i in range(len(aki_stage)):\n",
    "        if i + padding_hr + predict_after_hr > len(aki_stage) - 1:\n",
    "            break\n",
    "\n",
    "        # Selecting the appropriate AKI stage value for this window\n",
    "        aki_tmp = aki_stage[i + padding_hr + predict_after_hr]\n",
    "\n",
    "        uo_tmp = np.array(uo[i : i + padding_hr + 1])\n",
    "        time_tmp = np.array(charttime[i : i + padding_hr + 1])\n",
    "\n",
    "        uo_tmp = uo_tmp[:, np.newaxis]\n",
    "        time_tmp = time_tmp[:, np.newaxis]\n",
    "        x_tmp = np.hstack((uo_tmp, time_tmp))\n",
    "\n",
    "        X_per_patient.append(x_tmp)\n",
    "        Y_per_patient.append(aki_tmp)\n",
    "\n",
    "\n",
    "print(\"After processing, there are {} patients' data\".format(len(X_per_patient)))\n",
    "\n",
    "X_per_patient = np.array(X_per_patient, dtype=object)  # Use dtype=object for varying lengths\n",
    "Y_per_patient = np.array(Y_per_patient)\n",
    "\n",
    "print(X_per_patient.shape)\n",
    "print(Y_per_patient.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499542\n",
      "[[[0.0]]\n",
      "\n",
      " [[0.0]]\n",
      "\n",
      " [[0.0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0]]\n",
      "\n",
      " [[0.0]]\n",
      "\n",
      " [[1.0]]]\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_per_patient))\n",
    "print(Y_per_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2clfKAvmF8z",
    "outputId": "4d419e31-de2e-4e34-e1f1-8f60da7abfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2499542, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_per_patient = Y_per_patient[:, np.newaxis, np.newaxis]\n",
    "print(Y_per_patient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHNwhdbkUcBg",
    "outputId": "7658c1b4-88f4-4db5-f9c5-0629dd615140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "therea are 619198 windows with AKI\n",
      "therea are 1880344 windows without AKI\n"
     ]
    }
   ],
   "source": [
    "AKI_patient = 0\n",
    "noAKI_patient = 0\n",
    "for AKI_stages in Y_per_patient:\n",
    "    if 1 in AKI_stages or 2 in AKI_stages or 3 in AKI_stages:\n",
    "        AKI_patient += 1\n",
    "    else:\n",
    "        noAKI_patient += 1\n",
    "\n",
    "print(\"therea are {} windows with AKI\".format(AKI_patient))\n",
    "print(\"therea are {} windows without AKI\".format(noAKI_patient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygl6srrVUcBg"
   },
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "p9D2T_8hUcBg"
   },
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "X_per_patient, Y_per_patient = shuffle(X_per_patient, Y_per_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NhrnjRvnUcBh"
   },
   "outputs": [],
   "source": [
    "def splitData(X, Y, val_rate, test_rate):\n",
    "    X_val = X[ : int(X.shape[0]*val_rate)]\n",
    "    Y_val = Y[ : int(Y.shape[0]*val_rate)]\n",
    "\n",
    "    X_test = X[int(X.shape[0]*val_rate) : int(X.shape[0]*val_rate)+int(X.shape[0]*test_rate)]\n",
    "    Y_test = Y[int(Y.shape[0]*val_rate) : int(Y.shape[0]*val_rate)+int(Y.shape[0]*test_rate)]\n",
    "\n",
    "    X_train = X[int(X.shape[0]*val_rate)+int(X.shape[0]*test_rate) : ]\n",
    "    Y_train = Y[int(Y.shape[0]*val_rate)+int(Y.shape[0]*test_rate) : ]\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6y5FfyujUcBh",
    "outputId": "b2cb44ff-9b9a-4b57-96db-41261a806c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1749680, 24, 2)\n",
      "(1749680, 1, 1)\n",
      "(499908, 24, 2)\n",
      "(499908, 1, 1)\n",
      "(249954, 24, 2)\n",
      "(249954, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = splitData(X_per_patient, Y_per_patient, val_rate=0.2, test_rate=0.1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DXnbPqNUcBh"
   },
   "source": [
    "# build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6WulpnJFUcBh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0hwZByv-UcBh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 11:17:29.395521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1089 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 150)           91800     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 1, 150)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 150)            180600    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 150)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 1, 100)            100400    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 100)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 100)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 100)            0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 1, 2)             202       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533,802\n",
      "Trainable params: 533,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "def LSTMmodel():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=150, return_sequences = True, input_shape=((padding_hr + 1), 2)))\n",
    "    model.add(Lambda(lambda x: x[:, -1:, :])) # use Lambda() to  remove output of the first `padding_hr` nodes\n",
    "    #adding layer\n",
    "    model.add(LSTM(units=150, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(Dense(units = 1))\n",
    "\n",
    "    model.add(TimeDistributed((Dense(2, activation='softmax'))))\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #                 initial_learning_rate=1e-4,\n",
    "    #                 decay_steps=14677,\n",
    "    #                 decay_rate=0.9)\n",
    "\n",
    "    # Compiling\n",
    "    # regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    opt = Adam(learning_rate=0.001, clipvalue=1.0)\n",
    "    model.compile(optimizer=opt,\\\n",
    "                  loss='sparse_categorical_crossentropy',\\\n",
    "                  # loss='binary_crossentropy',\\ \n",
    "                  \n",
    "                  # when keras.losses.BinaryCrossentropy(from_logits=True) with dense unit = 1, accuracy become 0.35\n",
    "                  # change the last layer of LSTM units from 50 to 100 and change the loss function back to sparse_categorical, accu become 0.66 and \n",
    "                  # initial accu of the 24 hour with 5 layers is 0.71\n",
    "                  \n",
    "                  # loss = 'sparse_categorical_crossentropy',\\\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "LSTMmodel = LSTMmodel()\n",
    "LSTMmodel.summary()\n",
    "LSTMmodel.save_weights(\"checkpoint.weights.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91V4koeFUcBh",
    "outputId": "4d3ec3d6-557a-4384-96af-e9122729ceff"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m LRcallback \u001b[38;5;241m=\u001b[39m LearningRateScheduler(scheduler)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# LSTMmodel.load_weights(\"ckpt/checkpoint.weights.0.h5\")\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m history_callback \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEScallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCKPTcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr_on_plateau\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv_3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/myenv_3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "# This approach would typically be implemented with a ReduceLROnPlateau callback rather than a LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "gc.collect()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "EScallback = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "CKPTcallback = ModelCheckpoint(filepath=\"ckpt/checkpoint.weights.{epoch}.h5\", monitor='val_loss', \\\n",
    "                               verbose=1, save_best_only=True, save_weights_only=True, mode=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "RLcallback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Create a ReduceLROnPlateau callback\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # Monitor the validation loss\n",
    "    factor=0.1,            # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=5,            # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.0001,         # Lower bound on the learning rate\n",
    "    verbose=1              # Print messages when the callback takes an action\n",
    ")\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "LRcallback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# LSTMmodel.load_weights(\"ckpt/checkpoint.weights.0.h5\")\n",
    "\n",
    "history_callback = LSTMmodel.fit(X_train, Y_train, epochs=100, batch_size=128, validation_data=(X_val, Y_val), callbacks=[EScallback, CKPTcallback, reduce_lr_on_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Q1sY0PNnUcBh",
    "outputId": "9c3ba1a7-4afd-4d59-9599-30aff30dd1e7"
   },
   "outputs": [],
   "source": [
    "print(history_callback.history.keys())\n",
    "total_epochs = len(history_callback.history[\"loss\"])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=100)\n",
    "ax.set_title('Loss')\n",
    "ax.plot(range(total_epochs), history_callback.history[\"loss\"], label='Train')\n",
    "ax.plot(range(total_epochs), history_callback.history[\"val_loss\"], label='Valid')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('metrics.jpg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sdghka8UcBi"
   },
   "source": [
    "# try to evaluate by test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iheFsAVmUcBi"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jr_Hv5q_wYh2",
    "outputId": "920efca6-3968-4ee1-b286-5f30f990c253"
   },
   "outputs": [],
   "source": [
    "# LSTMmodel.load_weights(\"ckpt/checkpoint.{}.weights.h5\".format(121))\n",
    "LSTMmodel.load_weights(\"checkpoint.weights.0.h5\")\n",
    "y_pred = LSTMmodel.predict(X_test)\n",
    "\n",
    "y_pred_oneClass = np.argmax(y_pred, axis=2)\n",
    "Y_test_2D = Y_test.reshape((Y_test.shape[0], Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX1FTmRGvY72"
   },
   "source": [
    "### all time accuracy (with -1 padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKHM4otbxaDr",
    "outputId": "ce05f0a4-7443-41fe-f987-46b69211c5d6"
   },
   "outputs": [],
   "source": [
    "### 5 classes\n",
    "y_pred_flatten = y_pred_oneClass.reshape((-1))\n",
    "y_test_flatten = Y_test_2D.reshape((-1))\n",
    "\n",
    "print(classification_report(y_test_flatten, y_pred_flatten))\n",
    "print(accuracy_score(y_test_flatten, y_pred_flatten))\n",
    "print(confusion_matrix(y_test_flatten, y_pred_flatten))\n",
    "\n",
    "print(\"=================\")\n",
    "\n",
    "### 3 classes\n",
    "y_pred_flatten_3 = np.array(y_pred_flatten)\n",
    "y_test_flatten_3 = np.array(y_test_flatten)\n",
    "\n",
    "y_pred_flatten_3[y_pred_flatten_3 == 2] = 1\n",
    "y_pred_flatten_3[y_pred_flatten_3 == 3] = 1\n",
    "y_test_flatten_3[y_test_flatten_3 == 2] = 1\n",
    "y_test_flatten_3[y_test_flatten_3 == 3] = 1\n",
    "\n",
    "print(classification_report(y_test_flatten_3, y_pred_flatten_3))\n",
    "print(accuracy_score(y_test_flatten_3, y_pred_flatten_3))\n",
    "print(confusion_matrix(y_test_flatten_3, y_pred_flatten_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJQLR8gBTQ7Y"
   },
   "source": [
    "### without -1 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMCEr_NSRhbq",
    "outputId": "bf37c470-f903-4d80-dedd-dce69ea5dfe1"
   },
   "outputs": [],
   "source": [
    "### 5 classes\n",
    "y_pred_nopad = []\n",
    "y_test_nopad = []\n",
    "for yp, yt in zip(y_pred_oneClass, Y_test_2D):\n",
    "    yt_tmp = np.delete(yt, np.where(yt == 4))\n",
    "    yp_tmp = yp[: yt_tmp.shape[0]]\n",
    "\n",
    "    y_test_nopad = [*y_test_nopad, *yt_tmp]\n",
    "    y_pred_nopad = [*y_pred_nopad, *yp_tmp]\n",
    "\n",
    "\n",
    "print(classification_report(y_test_nopad, y_pred_nopad))\n",
    "print(accuracy_score(y_test_nopad, y_pred_nopad))\n",
    "print(confusion_matrix(y_test_nopad, y_pred_nopad))\n",
    "\n",
    "print(\"=============\")\n",
    "### 3 classes\n",
    "y_pred_nopad_3 = np.array(y_pred_nopad)\n",
    "y_test_nopad_3 = np.array(y_test_nopad)\n",
    "\n",
    "y_pred_nopad_3[y_pred_nopad_3 == 2] = 1\n",
    "y_pred_nopad_3[y_pred_nopad_3 == 3] = 1\n",
    "y_test_nopad_3[y_test_nopad_3 == 2] = 1\n",
    "y_test_nopad_3[y_test_nopad_3 == 3] = 1\n",
    "\n",
    "print(classification_report(y_test_nopad_3, y_pred_nopad_3))\n",
    "print(accuracy_score(y_test_nopad_3, y_pred_nopad_3))\n",
    "print(confusion_matrix(y_test_nopad_3, y_pred_nopad_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFiqxvDoUGkG"
   },
   "source": [
    "### without -1 padding, last 5 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ew_8RaiOUFw6",
    "outputId": "7ed1c60b-90f0-4bb1-8f92-f839722df652"
   },
   "outputs": [],
   "source": [
    "### 5 classes\n",
    "y_pred_last5 = []\n",
    "y_test_last5 = []\n",
    "for yp, yt in zip(y_pred_oneClass, Y_test_2D):\n",
    "    yt_tmp = np.delete(yt, np.where(yt == 4))\n",
    "    yp_tmp = yp[: yt_tmp.shape[0]]\n",
    "\n",
    "    y_test_last5 = [*y_test_last5, *yt_tmp[-5:]]\n",
    "    y_pred_last5 = [*y_pred_last5, *yp_tmp[-5:]]\n",
    "\n",
    "\n",
    "print(classification_report(y_test_last5, y_pred_last5))\n",
    "print(accuracy_score(y_test_last5, y_pred_last5))\n",
    "print(confusion_matrix(y_test_last5, y_pred_last5))\n",
    "\n",
    "print(\"=============\")\n",
    "### 3 classes\n",
    "y_pred_last5_3 = np.array(y_pred_last5)\n",
    "y_test_last5_3 = np.array(y_test_last5)\n",
    "\n",
    "y_pred_last5_3[y_pred_last5_3 == 2] = 1\n",
    "y_pred_last5_3[y_pred_last5_3 == 3] = 1\n",
    "y_test_last5_3[y_test_last5_3 == 2] = 1\n",
    "y_test_last5_3[y_test_last5_3 == 3] = 1\n",
    "\n",
    "print(classification_report(y_test_last5_3, y_pred_last5_3))\n",
    "print(accuracy_score(y_test_last5_3, y_pred_last5_3))\n",
    "print(confusion_matrix(y_test_last5_3, y_pred_last5_3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel_3",
   "language": "python",
   "name": "kernel_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
